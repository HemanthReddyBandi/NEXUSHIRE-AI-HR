# Quick Start Guide - Free LLM Interview Platform

## ‚ö° 3-Step Setup

### Step 1: Install Ollama (2 minutes)
- Download: https://ollama.ai/download
- Run the installer
- Restart PowerShell/Terminal

### Step 2: Download Model (5-10 minutes)
**Windows (PowerShell):**
```powershell
ollama pull mistral
```

**Mac/Linux:**
```bash
ollama pull mistral
```

### Step 3: Run the App
```bash
npm install    # (already done if you ran npm install before)
npm run dev
```

Open: **http://localhost:3000**

---

## ‚úÖ Verify Setup

Check Ollama is running:
```powershell
curl.exe http://localhost:11434/api/tags
```

Should see something like:
```json
{"models":[{"name":"mistral:latest", ...}]}
```

---

## üéØ That's It!

- Questions will now be **generated by local LLM**
- Answers will be **read aloud** by browser
- Feedback will be **provided by local LLM**
- Everything **stays on your machine** (offline capable)
- **No API keys** needed
 - If you prefer using Google Gemini (GenAI) instead of a local model, you can set a Google API key locally and use the built-in `services/geminiService.ts`.
 - To use Gemini, create a `.env` from `.env.example` and set `VITE_API_KEY` to your Google GenAI API key (do NOT commit this key).

---

## ‚ùì Troubleshooting

| Problem | Solution |
|---------|----------|
| "Connection refused" | Ensure Ollama is running |
| Still "Mock Question" | Restart dev server after Ollama starts |
| Slow questions | Model is loading first time. Wait or use smaller model |
| No voice | Check browser mic is enabled |

---

## üöÄ Advanced

### Use Different Model
```bash
ollama pull llama2
# Edit services/ollamaService.ts change "mistral" to "llama2"
npm run dev
```

### Run Setup Script (Auto-downloads model)
```bash
npm run setup:ollama
# or
.\setup-ollama.bat   # Windows
```

---

## üìö Full Docs

See `OLLAMA_SETUP.md` and `README.md` for detailed guides
